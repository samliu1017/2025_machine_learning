{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb89076",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc28c34",
   "metadata": {},
   "source": [
    "### 1.Evaluate $\\theta^1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6611be",
   "metadata": {},
   "source": [
    "根據題目中的模型： $h(x_1,x_2) = \\sigma(b+w_1x_1+w_2x_2)$, Sigmoid function: $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Consider MSE loss: $L=\\tfrac12(y-h)^2 = \\tfrac12(h-y)^2$\n",
    "\n",
    "And we find the gradient:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}=(h-y)\\sigma'(z), \\quad\n",
    "\\frac{\\partial L}{\\partial w_i}=(h-y)\\sigma'(z)\\,x_i\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1-\\sigma(z)), \\quad z=b+w_1x_1+w_2x_2\n",
    "$$\n",
    "\n",
    "根據題目，帶入數值 $(x_1,x_2,y)=(1,2,3)$， $\\theta^0 = (b,w_1,w_2)=(4,5,6)$。\n",
    "\n",
    "算出 $z=21$\n",
    "\n",
    "使用公式$\\theta ^1 = \\theta ^0 - \\alpha \\partial_\\theta Loss$\n",
    "\n",
    "可以求出 $\\theta^1 = (b^1, x_1^1, x_2^2)$ ：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "b^{1}   &= 4-\\alpha\\bigl(\\sigma(21)-3\\bigr)\\,\\sigma(21)\\bigl(1-\\sigma(21)\\bigr)\\cdot 1,\\\\[6pt]\n",
    "w_1^{1} &= 5-\\alpha\\bigl(\\sigma(21)-3\\bigr)\\,\\sigma(21)\\bigl(1-\\sigma(21)\\bigr)\\cdot 1,\\\\[6pt]\n",
    "w_2^{1} &= 6-\\alpha\\bigl(\\sigma(21)-3\\bigr)\\,\\sigma(21)\\bigl(1-\\sigma(21)\\bigr)\\cdot 2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53acc0e9",
   "metadata": {},
   "source": [
    "### 2.(a) Find the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e981af",
   "metadata": {},
   "source": [
    "Let $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "Then：\n",
    "\n",
    "$\\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(1-\\sigma)$\n",
    "\n",
    "\n",
    "\n",
    "$\\sigma''(x) = \\frac{d}{dx} [\\sigma(1-\\sigma)] = \\sigma'[1-2\\sigma] = \\sigma(1-\\sigma)(1-2\\sigma)$\n",
    "\n",
    "\n",
    "$\\sigma^{(3)}(x) = \\frac{d}{dx}[\\sigma''] = \\frac{d}{dx}[2\\sigma^3 -3\\sigma^2 + \\sigma] = \\sigma'(6\\sigma^2 -6 \\sigma + 1) = \\sigma(1-\\sigma)(6\\sigma^2 -6 \\sigma + 1)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f460a4b",
   "metadata": {},
   "source": [
    "### 2.(b) Find the relation between sigmoid function and hyperbolic function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63f5be",
   "metadata": {},
   "source": [
    "The sigmoid function is $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "The hyperbolic sin function is $\\sinh(x) = \\frac{e^x - e^{-x}}{2}$.\n",
    "\n",
    "The hyperbolic cos function is $\\cosh(x) = \\frac{e^x + e^{-x}}{2}$ .\n",
    "\n",
    "The hyperbolic tan function is $\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$.\n",
    "\n",
    "\n",
    "Because $\\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^{x}}{1+e^{x}} = 1 - \\frac{1}{1+e^{x}}$\n",
    "\n",
    "So, $ e^{x} = \\frac{1}{1-\\sigma} -1 = \\frac{\\sigma}{1-\\sigma}$\n",
    "#### (1)sinh\n",
    "\n",
    "$$\\sinh(x) = \\frac{e^x - e^{-x}}{2} = \\frac{\\frac{\\sigma}{1-\\sigma} - \\frac{1-\\sigma}{\\sigma}}{2} = \\frac{2\\sigma-1}{2\\sigma(1-\\sigma)}$$\n",
    "\n",
    "#### (2)cosh\n",
    "\n",
    "$$\\cosh(x) = \\frac{e^x + e^{-x}}{2} = \\frac{\\frac{\\sigma}{1-\\sigma} + \\frac{1-\\sigma}{\\sigma}}{2} = \\frac{2\\sigma^2 - 2\\sigma + 1}{2\\sigma(1-\\sigma)}$$\n",
    "\n",
    "\n",
    "#### (3)tanh\n",
    "$$\n",
    "\\tanh\\left(x\\right) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(2x) = \\frac{1}{1+e^{-2x}} = \\frac{e^{2x}}{e^{2x} + 1}, \\qquad 2\\sigma(2x) - 1 = \\frac{2e^{2x}}{e^{2x} + 1}-\\frac{e^{2x}+1}{e^{2x} + 1} = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "$$\n",
    "\n",
    "\n",
    "So,\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{2}\\left(1 + \\tanh\\left(\\frac{x}{2}\\right)\\right), \\qquad \\tanh\\left(x\\right) = 2\\sigma(2x) - 1\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c58e1a",
   "metadata": {},
   "source": [
    "### 3. Some Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e4873",
   "metadata": {},
   "source": [
    "#### Q1. Why are most models linear models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a7f73",
   "metadata": {},
   "source": [
    "##### Answer 1 (By Gemini)\n",
    "\n",
    "\n",
    "Key Advantages of Linear Models\n",
    "\n",
    "Linear models are valuable for several reasons, which make them a popular choice for a wide range of applications:\n",
    "\n",
    "$(1)$ Simplicity and Interpretability\n",
    "\n",
    "$(2)$ Computational Efficiency\n",
    "\n",
    "$(3)$ Well-Understood Statistical Properties\n",
    "\n",
    "$(4)$ Foundation for More Complex Models\n",
    "\n",
    "\n",
    "When to Use a Linear Model\n",
    "\n",
    "A linear model is often the best choice in the following situations:\n",
    "\n",
    "$(1)$ When the relationship is likely linear\n",
    "\n",
    "$(2)$ For baseline performance\n",
    "\n",
    "$(3)$ When interpretability is a priority\n",
    "\n",
    "Limitations of Linear Models\n",
    "\n",
    "Their main limitation is their assumption of a linear relationship between the dependent and independent variables.\n",
    "If the underlying data has a complex, non-linear pattern, a linear model will likely fail to capture it accurately. \n",
    "This can lead to underfitting and poor predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebf03e",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between BGD, SGD, and Mini-Batch Gradient Descent, and what are their advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6eaabb",
   "metadata": {},
   "source": [
    "\n",
    "##### Answer 2 (By Chat GPT)\n",
    "$(1)$ Batch Gradient Descent (BGD)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Stable convergence because the gradient is computed using all data.\n",
    "\n",
    "Smooth and predictable updates.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Very slow on large datasets since every step requires going through all data.\n",
    "\n",
    "Not suitable for online or streaming data.\n",
    "\n",
    "Use Case: Small datasets\n",
    "\n",
    "$(2)$ Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Much faster per update because it uses only one example.\n",
    "\n",
    "Can handle very large datasets or streaming data.\n",
    "\n",
    "Can escape local minima more easily due to noisy updates.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Updates are noisy and can fluctuate heavily.\n",
    "\n",
    "Convergence is less stable; may require learning rate decay.\n",
    "\n",
    "Use Case: Very large/streaming data\n",
    "\n",
    "$(3)$ Mini-Batch Gradient Descent\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Faster than BGD because it doesn’t need the full dataset.\n",
    "\n",
    "Less noisy than SGD; more stable convergence.\n",
    "\n",
    "Can exploit optimized matrix operations on hardware (like GPUs).\n",
    "\n",
    "Strikes a balance between speed and accuracy.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Still requires tuning of batch size.\n",
    "\n",
    "Can have some noise in gradient estimate, though usually beneficial.\n",
    "\n",
    "Use Case: Most deep learning tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
