{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192f6101",
   "metadata": {},
   "source": [
    "### Introduce to Sobolev space\n",
    "#### Background\n",
    "在研究偏微分方程中，數學家們發現$C^k Space$與$L^p Space$不夠理想，於是Sobolev space出現，被用來研究偏微分方程。\n",
    "\n",
    "#### Definition of Sobolev space\n",
    "\n",
    "$$\n",
    "W^{k,p}(\\Omega) = \\{\\, u \\in L^p(\\Omega) \\;:\\; D^\\alpha u \\in L^p(\\Omega), \\; |\\alpha|\\leq k \\,\\}.\n",
    "$$\n",
    "\n",
    "#### Symbol meaning\n",
    "\n",
    "$\\Omega$ ： $u$ 的定義域。\n",
    "\n",
    "\n",
    "$u$ ： 定義在$\\Omega$的函數。\n",
    "\n",
    "$W^{k,p}(\\Omega)$ ： Sobolev Space.\n",
    "\n",
    "$L^p(\\Omega)$ ： $L^p$ Space. Where $u \\in L^p(\\Omega) \\iff \\int_{\\Omega} |u(x)|^p \\, dx < \\infty$\n",
    "\n",
    "$D^\\alpha u$ ： 函數$u$的弱微分\n",
    "\n",
    "{\n",
    "另 $u \\in L^1_{\\text{loc}}(\\Omega)$。\n",
    "\n",
    "若存在$v \\in L^1_{\\text{loc}}(\\Omega)$使得對所有測試函數$\\phi \\in C_c^\\infty(\\Omega)$ 有：\n",
    "\n",
    "$\n",
    "\\int_{\\Omega} u(x) \\, \\phi'(x) \\, dx = - \\int_{\\Omega} v(x) \\, \\phi(x) \\, dx,\n",
    "$\n",
    "\n",
    "則稱$v$為$u$的弱導數，記作$v = u'$.\n",
    "}\n",
    "\n",
    "---\n",
    "$|\\alpha| \\leq k$ ：\n",
    "\n",
    "\n",
    "$\\alpha = (\\alpha_1,\\alpha_2,\\alpha_3...\\alpha_n), \\quad \\alpha_i \\geq 0.\\qquad$ #$\\alpha$ is a Multi-index notation.\n",
    "\n",
    "$\n",
    "D^\\alpha u \n",
    "= \\frac{\\partial^{|\\alpha|} u}{\\partial x_1^{\\alpha_1} \\partial x_2^{\\alpha_2} \\partial x_3^{\\alpha_3}...\\partial x_n^{\\alpha_n}}, \n",
    "\\quad |\\alpha| = \\alpha_1 + \\alpha_2 + \\alpha_3 + ... + \\alpha_n.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Definition of Sobolev seminorm\n",
    "$$\n",
    "|u|_{W^{m,p}(\\Omega)} \n",
    "= \\left( \\sum_{|\\alpha|= m} \\| D^\\alpha u \\|_{L^p(\\Omega)}^p \\right)^{1/p},\n",
    "\\qquad 1 \\leq p < \\infty.\n",
    "$$\n",
    "\n",
    "$$\n",
    "|u|_{W^{m,\\infty}(\\Omega)} \n",
    "= \\max_{|\\alpha|= m} \\| D^\\alpha u \\|_{L^\\infty(\\Omega)}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Definition of Sobolev norm\n",
    "$$\n",
    "\\|u\\|_{W^{m,p}(\\Omega)} \n",
    "= \\left( \\sum_{|\\alpha|\\leq m} \\| D^\\alpha u \\|_{L^p(\\Omega)}^p \\right)^{1/p},\n",
    "\\qquad 1 \\leq p < \\infty.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\|u\\|_{W^{m,\\infty}(\\Omega)} \n",
    "= \\max_{|\\alpha|\\leq m} \\| D^\\alpha u \\|_{L^\\infty(\\Omega)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2dabf",
   "metadata": {},
   "source": [
    "### Introduce to Paper\n",
    "#### lemma[3.1]\n",
    "\n",
    "Let $k \\in \\mathbb{N}_0$ and $s \\in 2\\mathbb{N} - 1$. \n",
    "\n",
    "Then it holds that for all \n",
    "$\\epsilon > 0$ there exists a shallow tanh neural network \n",
    "$\\Psi_{s,\\epsilon} : [-M, M] \\to \\mathbb{R}^{\\tfrac{s+1}{2}}$ \n",
    "of width $\\tfrac{s+1}{2}$ such that\n",
    "$$\n",
    "    \\max_{\\substack{p \\leq s, \\\\ p \\text{ odd}}} \n",
    "    \\left\\| f_p - \\big( \\Psi_{s,\\epsilon} \\big)_{\\tfrac{p+1}{2}} \\right\\|_{W^{k,\\infty}}\n",
    "    \\leq \\epsilon,\n",
    "$$\n",
    "Moreover, the weights of $\\Psi_{s,\\epsilon}$ scale as\n",
    "$\n",
    "    O\\!\\left( \n",
    "        \\epsilon^{-s/2} \\big( 2(s+2)\\sqrt{2M} \\big)^{s(s+3)}\n",
    "    \\right)\n",
    "$\n",
    "for small $\\epsilon$ and large $s$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb18dfa",
   "metadata": {},
   "source": [
    "#### Explains and Ideas\n",
    "\n",
    "Ideas:\n",
    "\n",
    "Lemma(3.1)的目標是建立一個淺層(1 hidden layer) $tanh$神經網路，寬度為$\\frac{s+1}{2}$。這個神經網路可以逼近所有奇數次的多項式。\n",
    "\n",
    "Translate:\n",
    "\n",
    "在區間[M,-M]之間，s是正奇數。對於所有$\\epsilon > 0$時，必定存在一個淺層tanh神經網路$\\Psi_{s,\\epsilon}$，使得$\\Psi_{s,\\epsilon}$非常靠近奇數次多項式$f_p$。 \\\n",
    "同時weights of $\\Psi_{s,\\epsilon}$ scale為\n",
    "$O\\!\\left( \\epsilon^{-s/2} \\big( 2(s+2)\\sqrt{2M} \\big)^{s(s+3)}\\right)$ 當$s$很大且$\\epsilon$很小。\n",
    "\n",
    "Proof:(粗略證明)\n",
    "透過使用Taylor thm, Katsuura(2009,Thm1), Lemmas A.1, A.4, Stirling’s approximation。最後得到對所有隨意的正整數$k$都有     \\\n",
    "$\n",
    "\\| f_p - \\hat{f}_{p,h} \\|_{W^{k,\\infty}} \n",
    "\\le \n",
    "\\Big( (2(p+2)p M)^{p+3} + (2 p k)^{k+1} \\Big) h^2\n",
    "=:\\epsilon.\n",
    "$\n",
    "\n",
    "Remark:\n",
    "\n",
    "對所有偶數次多項式，可以把它寫成     \\\n",
    "$y^{2n} = \\frac{1}{2\\alpha (2n+1)} ((y+\\alpha)^{2n+1} - (y-\\alpha)^{2n+1} - 2 \\sum_{k=0}^{n-1} \\binom{2n+1}{2k} \\alpha^{2(n-k)+1} y^{2k})$    \\\n",
    "每個也都能透過tanh神經網路進行逼近。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3777b1d",
   "metadata": {},
   "source": [
    "#### lemma[3.2]\n",
    "\n",
    "Let $k \\in \\mathbb{N}_0$, $s \\in 2\\mathbb{N}-1$, and $M>0$. \n",
    "For every $\\epsilon>0$, there exists a shallow $\\tanh$ neural network \n",
    "$\\psi_{s,\\epsilon} : [-M,M] \\to \\mathbb{R}^s$ of width $\\frac{3(s+1)}{2}$ such that\n",
    "$\\max_{p \\le s} \n",
    "\\| f_p - (\\psi_{s,\\epsilon})_p \\|_{W^{k,\\infty}} \\le \\epsilon.$\n",
    "Furthermore, the weights scale as\n",
    "$\n",
    "O\\Big( \\epsilon^{-s/2} (\\sqrt{M(s+2)})^{\\frac{3s(s+3)}{2}} \\Big)\n",
    "$\n",
    "for small $\\epsilon$ and large $s$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576cae63",
   "metadata": {},
   "source": [
    "#### Explains and Ideas\n",
    "\n",
    "Ideas:\n",
    "\n",
    "Lemma(3.2)會建立一個淺層(1 hidden layer) $tanh$神經網路，寬度為$\\frac{3(s+1)}{2}$。這個神經網路可以逼近所有多項式。\n",
    "\n",
    "Translate:\n",
    "\n",
    "在區間[M,-M]之間，s是正奇數。對於所有$\\epsilon > 0$時，必定存在一個淺層tanh神經網路$\\Psi_{s,\\epsilon}$，使得$\\Psi_{s,\\epsilon}$非常靠近多項式$f_p$。 \\\n",
    "同時weights scale為\n",
    "$O\\Big( \\epsilon^{-s/2} (\\sqrt{M(s+2)})^{\\frac{3s(s+3)}{2}} \\Big)$ 當$s$很大且$\\epsilon$很小。\n",
    "\n",
    "Proof:(粗略證明)\n",
    "將偶數次多項式寫成上方remark形式，變成許多奇數次多項式。\\\n",
    "同時定義$E_p = \\max_{p \\le s} \\; || f_p - (\\psi_{s,\\epsilon})_p ||_{W^{k,\\infty}} \\le \\epsilon$ \\\n",
    "又根據Lemma3.1得到$\\max_{\\substack{p \\le s \\\\ p \\ \\text{odd}}} \\; E_p \\le \\epsilon$\n",
    "\n",
    "最後透過induction得到\n",
    "$\\max_{p \\le s} \n",
    "\\left\\lVert f_p - (\\psi_{s,\\epsilon})_p \\right\\rVert_{W^{k,\\infty}}\n",
    "\\;\\le\\; \\sqrt{e}\\,(2es)^{s/2}\\,\\epsilon.$\\\n",
    "Replacing $\\epsilon \\to \\dfrac{\\epsilon}{\\sqrt{e}\\,(2es)^{s/2}}$即完成證明。\n",
    "\n",
    "\n",
    "Remark:\n",
    "偶數次hidden layer為\n",
    "$\n",
    "\\sigma\\!\\left( \\left( \\tfrac{s}{2} - i \\right) h \\, (y + \\beta) \\right),\n",
    "\\quad \\text{where } i = 0,1,\\dots, \\tfrac{s-1}{2}, \\; \\beta \\in \\{-\\alpha, 0, \\alpha\\}.\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef735b1c",
   "metadata": {},
   "source": [
    "### Applying\n",
    "\n",
    "\n",
    "Choose $f(x)=x^5+x^4+x^3+x^2+x+1$ ，$x\\in[-1,1]$\n",
    "\n",
    "1. **Choose $s$**  \n",
    "   因為 $f(x)$ 的最高次是 5，所以取 $s=5$（奇數）。  \n",
    "\n",
    "2. **使用 Lemma 3.2**  \n",
    "   Lemma 3.2 告訴我們：對任意精度 $\\epsilon>0$，存在一個淺層神經網路 $\\tanh$ \\\n",
    "  $\\psi_{5,\\epsilon} : [-1,1]\\to\\mathbb{R}^5$\n",
    "   寬度為 $\\tfrac{3(s+1)}{2} = 9$，可以同時逼近所有 $x^p,(p=1,2,3,4,5)$。  \n",
    "\n",
    "3. **組合多項式**  \n",
    "   由於 $f(x)=x^5+x^4+x^3+x^2+x+1=\\sum_{p=0}^5 x^p$  \\\n",
    "   我們只需要把$(\\psi_{5,\\epsilon})_p$ 相加，再加上常數 1，就得到對 $f(x)$ 的近似：\n",
    "   $\\tilde f(x) = \\sum_{p=0}^5 (\\psi_{5,\\epsilon})_p(x).$ \n",
    "\n",
    "4. **誤差控制**  \n",
    "   每個單項式的逼近誤差 $\\le \\delta$。總共有 6 項，所以  \n",
    "   $| f - \\tilde f \\|_{W^{k,\\infty}} \\le 6\\delta$\n",
    "   取 $\\delta=\\varepsilon/6$。  \n",
    "\n",
    "5. **網路規模與權重大小**  \n",
    "   網路寬度固定為 9（很小）。  \n",
    "   權重大小大約會隨 $\\varepsilon^{-5/2}$ 增長）。  \n",
    "\n",
    "---\n",
    "\n",
    "用 Lemma 3.2，我們可以建構一個 width=9的淺層 $\\tanh$ 網路，能在區間 $[-1,1]$ 上逼近  $f(x)=x^5+x^4+x^3+x^2+x+1$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
