{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0476ced7",
   "metadata": {},
   "source": [
    "## 1.Calculate $\\nabla a^{[L]} (x)$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c649ea3",
   "metadata": {},
   "source": [
    "We have \n",
    "$$a^{[1]}=x \\in \\mathbb{R} ^ {n_1} \\qquad (3.1)$$\n",
    "$$a^{[l]}=\\sigma (W^{[l]}a^{[l-1]} + b^{[l]})=x \\in \\mathbb{R} ^ {n_l} \\quad for \\quad l = 2, 3, ... L\\qquad (3.2) $$\n",
    "$$ n_{L}=1 \\qquad Calculate \\quad \\nabla a^{[L]} (x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87c014",
   "metadata": {},
   "source": [
    "---\n",
    "### Calculate\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6f16a",
   "metadata": {},
   "source": [
    "Let $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\quad then \\quad a^{[l]}=\\sigma (z^{[l]})$ \n",
    "\n",
    "Let $ \\delta ^{[l]} = \\frac {\\partial a^{[L]}}{\\partial z^{[l]}} \\quad$ we have $ \\delta ^{[L]} = \\sigma '(z^{[L]})$\n",
    "\n",
    "For $2≤l≤L$ By chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a^{[L]}}{\\partial z^{[l]}}\n",
    "= \\frac{\\partial a^{[L]}}{\\partial z^{[l+1]}}\n",
    "\\cdot \\frac{\\partial z^{[l+1]}}{\\partial a^{[l]}}\n",
    "\\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}}.\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\delta^{[l+1]} = \\frac{\\partial a^{[L]}}{\\partial z^{[l+1]}}\n",
    "\\qquad \n",
    "W^{[l+1]} = \\frac{\\partial z^{[l+1]}}{\\partial a^{[l]}}  \n",
    "\\qquad \n",
    "\\frac{\\partial a_i^{[l]}}{\\partial z_j^{[l]}} =\n",
    "\\begin{cases}\n",
    "\\sigma'(z_i^{[l]}), & i=j\\\\[1mm]\n",
    "0, & i \\neq j\n",
    "\\end{cases}\n",
    "\\qquad so \\qquad \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} = \\operatorname{diag}(\\sigma'(z^{[l]})).\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\delta^{[l]} \n",
    "= \\sigma'(z^{[l]}) \\;\\circ\\; \\big((W^{[l+1]})^T \\delta^{[l+1]}\\big)\n",
    "$$\n",
    "\n",
    "Finally we get:\n",
    "$$\n",
    "\\nabla a^{[L]} = (W^{[2]})^T \\delta^{[2]} \n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0068040",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467445c",
   "metadata": {},
   "source": [
    "#### Input and Output\n",
    "##### Input\n",
    "Input vector $x \\in \\mathbb{R}^{n_1}$\n",
    "\n",
    "Network parameters $\\{ W^{[l]}, b^{[l]} \\}_{l=2}^L$\n",
    "\n",
    "Activation function $\\sigma(\\cdot)$\n",
    "\n",
    "##### Output\n",
    "\n",
    "Gradient $\\nabla a^{[L]}(x) $\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1\n",
    "\n",
    "$$\n",
    "Calculate \\quad \\delta^{[L]} = \\sigma'(z^{[L]})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2\n",
    "\n",
    "For $l = L-1, L-2, \\dots, 2$:\n",
    "$$\n",
    "\\delta^{[l]} = \\sigma'(z^{[l]}) \\circ \\Big( (W^{[l+1]})^T \\delta^{[l+1]} \\Big)\n",
    "$$\n",
    "\n",
    "where $\\circ$ denotes elementwise multiplication.\n",
    "\n",
    "Use this to compute $\\delta^{[2]}$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3\n",
    "\n",
    "$$ \\nabla a^{[L]} = (W^{[2]})^T \\delta^{[2]} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4\n",
    "\n",
    "$$\n",
    "\\text{Return } \\nabla a^{[L]}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafb1ba",
   "metadata": {},
   "source": [
    "## 2.Some Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021573b3",
   "metadata": {},
   "source": [
    "### Q1:Does having more layers or more neurons in a neural network lead to a better result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46e2de",
   "metadata": {},
   "source": [
    "Adding more layers (depth) or more neurons per layer (width) both increase the capacity of a neural network, but the effect on performance depends on the task, data, and optimization process.\n",
    "\n",
    "1.More layers (depth)\n",
    "\n",
    "Pros: Capture complex, hierarchical features.\n",
    "\n",
    "Cons: Harder to train, higher compute cost.\n",
    "\n",
    "2.More neurons (width)\n",
    "\n",
    "Pros: Increases expressiveness, easier to train.\n",
    "\n",
    "Cons: More parameters, risk of overfitting.\n",
    "\n",
    "3.Right Model Size\n",
    "\n",
    "Too few parameters → underfitting.\n",
    "\n",
    "Too many parameters → overfitting.\n",
    "\n",
    "It depends on:\n",
    "\n",
    "(1)Dataset size/complexity.\n",
    "\n",
    "(2)Compute budget.\n",
    "\n",
    "(3)Regularization methods .\n",
    "\n",
    "4.Key idea\n",
    "\n",
    "If your model underfits → add more layers/neurons.\n",
    "\n",
    "If your model overfits → reduce size, or add regularization / more data.\n",
    "\n",
    "If training is unstable → depth might be too large without proper techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
