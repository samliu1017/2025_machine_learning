{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920949b7",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c05727",
   "metadata": {},
   "source": [
    "#### Q1.Why do we usually begin with a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018194c",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "\n",
    "Key Advantages of Linear Models\n",
    "\n",
    "Linear models are valuable for several reasons, which make them a popular choice for a wide range of applications:\n",
    "\n",
    "$(1)$ Simplicity and Interpretability\n",
    "\n",
    "$(2)$ Computational Efficiency\n",
    "\n",
    "$(3)$ Well-Understood Statistical Properties\n",
    "\n",
    "$(4)$ Foundation for More Complex Models\n",
    "\n",
    "\n",
    "When to Use a Linear Model\n",
    "\n",
    "A linear model is often the best choice in the following situations:\n",
    "\n",
    "$(1)$ When the relationship is likely linear\n",
    "\n",
    "$(2)$ For baseline performance\n",
    "\n",
    "$(3)$ When interpretability is a priority\n",
    "\n",
    "Limitations of Linear Models\n",
    "\n",
    "Their main limitation is their assumption of a linear relationship between the dependent and independent variables.\n",
    "If the underlying data has a complex, non-linear pattern, a linear model will likely fail to capture it accurately. \n",
    "This can lead to underfitting and poor predictive performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf9420",
   "metadata": {},
   "source": [
    "#### Q2.What is the difference between BGD, SGD, and Mini-Batch Gradient Descent, and what are their advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6afa27",
   "metadata": {},
   "source": [
    "\n",
    "##### Answer\n",
    "$(1)$ Batch Gradient Descent (BGD)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Stable convergence because the gradient is computed using all data.\n",
    "\n",
    "Smooth and predictable updates.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Very slow on large datasets since every step requires going through all data.\n",
    "\n",
    "Not suitable for online or streaming data.\n",
    "\n",
    "Use Case: Small datasets\n",
    "\n",
    "$(2)$ Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Much faster per update because it uses only one example.\n",
    "\n",
    "Can handle very large datasets or streaming data.\n",
    "\n",
    "Can escape local minima more easily due to noisy updates.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Updates are noisy and can fluctuate heavily.\n",
    "\n",
    "Convergence is less stable; may require learning rate decay.\n",
    "\n",
    "Use Case: Very large/streaming data\n",
    "\n",
    "$(3)$ Mini-Batch Gradient Descent\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Faster than BGD because it doesn’t need the full dataset.\n",
    "\n",
    "Less noisy than SGD; more stable convergence.\n",
    "\n",
    "Can exploit optimized matrix operations on hardware (like GPUs).\n",
    "\n",
    "Strikes a balance between speed and accuracy.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Still requires tuning of batch size.\n",
    "\n",
    "Can have some noise in gradient estimate, though usually beneficial.\n",
    "\n",
    "Use Case: Most deep learning tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383078ee",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7529ff",
   "metadata": {},
   "source": [
    "### Q1.Does having more layers or more neurons in a neural network lead to a better result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13798e",
   "metadata": {},
   "source": [
    "Adding more layers (depth) or more neurons per layer (width) both increase the capacity of a neural network, but the effect on performance depends on the task, data, and optimization process.\n",
    "\n",
    "1.More layers (depth)\n",
    "\n",
    "Pros: Capture complex, hierarchical features.\n",
    "\n",
    "Cons: Harder to train, higher compute cost.\n",
    "\n",
    "2.More neurons (width)\n",
    "\n",
    "Pros: Increases expressiveness, easier to train.\n",
    "\n",
    "Cons: More parameters, risk of overfitting.\n",
    "\n",
    "3.Right Model Size\n",
    "\n",
    "Too few parameters → underfitting.\n",
    "\n",
    "Too many parameters → overfitting.\n",
    "\n",
    "It depends on:\n",
    "\n",
    "(1)Dataset size/complexity.\n",
    "\n",
    "(2)Compute budget.\n",
    "\n",
    "(3)Regularization methods .\n",
    "\n",
    "4.Key idea\n",
    "\n",
    "If your model underfits → add more layers/neurons.\n",
    "\n",
    "If your model overfits → reduce size, or add regularization / more data.\n",
    "\n",
    "If training is unstable → depth might be too large without proper techniques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c60f",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82666d52",
   "metadata": {},
   "source": [
    "#### Q1.In class we can approximate every polynoimal by using neural network. Can we use same way to approximate other function e.g. sin(x), cos(x) by using taylor expansion? What may happened, and what is its result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52054f2d",
   "metadata": {},
   "source": [
    "The Result: What happens? (The Problem)\n",
    "If you train a standard neural network to approximate $\\sin(x)$ by mimicking a polynomial (or simply training it on data near $x=0$), three major issues arise:\n",
    "\n",
    "A. Local Accuracy vs. Global Divergence\\\n",
    "Taylor series are approximations centered around a specific point (usually $x=0$).\n",
    "\n",
    "Result: The network will be very accurate near $x=0$.\\\n",
    "Failure: As $x$ moves away from zero (e.g., $x=100$), the polynomial terms (like $x^7$) dominate and shoot towards $+\\infty$ or $-\\infty$. However, the real $\\sin(x)$ stays bounded between $[-1, 1]$.\n",
    "\n",
    "B. Failure to Extrapolate (The Extrapolation Problem)\\\n",
    "This is a fundamental limitation in Deep Learning. Most standard neural networks (e.g., those using ReLU activation functions) are piecewise linear.\\\n",
    "Result: If you train the network on the range $[-\\pi, \\pi]$, it will fit the wave perfectly. However, if you ask it to predict $x = 10\\pi$, it will not repeat the wave pattern. It will simply continue the linear slope from the last point it saw. It cannot \"learn\" the concept of periodicity through polynomials.\n",
    "\n",
    "C. Inefficiency\\\n",
    "To approximate many cycles of a sine wave need an incredibly high-degree polynomial. This requires a very large and deep neural network, which is computationally inefficient compared to using a periodic activation function.\n",
    "\n",
    "References: \\\n",
    "**Ziyin, L., Hartwig, T., & Ueda, M. (2020). Neural networks fail to learn periodic functions and how to fix it. Advances in Neural Information Processing Systems, 33.**\n",
    "\n",
    "**Morra, S., Tosello, F., & Zoso, D. (2022). NN-Poly: Approximating common neural networks with Taylor polynomials. Frontiers in Robotics and AI, 9.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553a94f",
   "metadata": {},
   "source": [
    "### Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3263e91",
   "metadata": {},
   "source": [
    "#### Q1.在不同情況下 Generative model 與 discriminative model 表現情況不同，甚麼情況下Generative model 比 discriminative model 好/不好?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca2cd8",
   "metadata": {},
   "source": [
    "When is Generative \"Better\"? \\\n",
    "A. Small Data Regimes (Few Training Examples) \\\n",
    "Because the Discriminative model is prone to overfitting or simply cannot find the boundary yet.\n",
    "B. Missing Data\\\n",
    "Generative model can marginalize over the missing variables to still make a prediction.\\\n",
    "C. Out-of-Distribution (OOD) & Anomaly Detection\\\n",
    "Because Generative models learn what \"normal\" data looks like ($P(X)$), they can tell you if a new input is weird.\n",
    "D. Unsupervised / Semi-Supervised Learning\n",
    "Generative models can easily use unlabeled data to improve their understanding of $P(X)$. Discriminative models generally require every $X$ to have a label $Y$.\n",
    "\n",
    "When is Discriminative is \"Better\"?\\\n",
    "A. Large Data Regimes\n",
    "With enough data, they can find a more precise solution.\\\n",
    "B. Complex Feature Correlations\\\n",
    "If the relationship between input features is complex (e.g., in image pixels), simple Generative models (like Naive Bayes) fail because their assumptions (independence) are violated. Discriminative models (like CNNs) don't care about the distribution; they just learn the complex mapping $X \\to Y$.\n",
    "\n",
    "\n",
    "\n",
    "References: \\\n",
    "**Ng, A. Y., & Jordan, M. I. (2001). On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems, 14.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e2145",
   "metadata": {},
   "source": [
    "### Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca3f0c",
   "metadata": {},
   "source": [
    "#### Q1.What may happened when the if the real-world data we used doesn't satisfied normal distribution in GDA model? What is the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad262292",
   "metadata": {},
   "source": [
    "A. The \"Multimodal\" Failure (Catastrophic) \\\n",
    "The GDA Result: GDA tries to fit a single Gaussian \"bell curve\" over these two peaks. It will calculate the mean ($\\mu$) to be exactly in the middle of the two clusters—where no actual data exists.\\\n",
    "Consequence: The model predicts the highest probability for a region where the data density is actually zero.\n",
    "\n",
    "B. The \"Heavy Tail\" / Outlier Problem\n",
    "The GDA Result: GDA estimates parameters using Mean and Variance. These calculations are not robust. A single extreme outlier will significantly shift the calculated Mean and inflate the Covariance matrix.\\\n",
    "Consequence: The decision boundary shifts dramatically toward the outlier to accommodate it, degrading accuracy for the majority of \"normal\" points.\n",
    "\n",
    "C. Asymptotic Error (The Theoretical Limit)\n",
    "If assumptions hold: GDA is asymptotically efficient (it requires the least amount of data to reach the truth).\\\n",
    "If assumptions fail: GDA approaches a biased limit. Even if you give it 1,000,000 data points, it will converge to a wrong solution.\\\n",
    "Contrast: Discriminative models (like Logistic Regression) make fewer assumptions. As data increases, they will eventually find the correct boundary even if the data is not Gaussian.\n",
    "\n",
    "References: \\\n",
    "**Ng, A. Y., & Jordan, M. I. (2001). On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems, 14.**\n",
    "\n",
    "**Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.**\n",
    "\n",
    "**Hubert, M., & Van Driessen, K. (2004). Fast and robust discriminant analysis. Computational Statistics & Data Analysis, 45(2).**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eced063",
   "metadata": {},
   "source": [
    "### Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01592790",
   "metadata": {},
   "source": [
    "#### Q1.數據是高斯分布情況下使用GDA或Logistic regression進行分析，哪個結果會比較準確? 若數據稍微偏離一些結果會是甚麼樣子?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd41047",
   "metadata": {},
   "source": [
    "Scenario A: The Data is Perfectly Gaussian\\\n",
    "Winner: GDA\\\n",
    "Why?\\\n",
    "GDA uses the strong assumption of normality to \"fill in the gaps\" where data is scarce. Logistic Regression ignores this distributional information.\n",
    "\n",
    "Scenario B: The Data Deviates Slightly\\\n",
    "Winner: Logistic Regression\\\n",
    "Why?\\\n",
    "As the dataset grows, Logistic Regression will eventually outperform GDA because GDA is stuck with a \"modeling error\" (Bias), whereas Logistic Regression can adjust to the data's actual shape.\n",
    "\n",
    "\n",
    "References: \\\n",
    "**Efron, B. (1975). The efficiency of logistic regression compared to normal discriminant analysis. Journal of the American Statistical Association, 70(352).**\n",
    "\n",
    "**Ng, A. Y., & Jordan, M. I. (2001). On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems, 14.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54d282",
   "metadata": {},
   "source": [
    "\n",
    "#### Q2.有甚麼方法可以調整GDA的決策邊界，讓其形狀能夠改變成不同樣子？有辦法變成不規則曲線嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b43cb",
   "metadata": {},
   "source": [
    "A. Mixture Discriminant Analysis (MDA) - The key to \"Irregular\" shapes\\\n",
    "If you want the boundary to be an irregular or wavy curve, standard GDA (one Gaussian per class) is not enough. You must use Mixture Models.\\\n",
    "Logic: $P(X|Y=A) \\approx \\sum w_i \\cdot \\mathcal{N}(\\mu_i, \\Sigma_i)$.\\\n",
    "Result: The decision boundary becomes the complex intersection of these multiple \"hills.\" This allows the boundary to be highly irregular, disconnected, or wobbly, effectively approximating any complex shape.\\\n",
    "\n",
    "B. Kernel GDA (Generalized Discriminant Analysis)\n",
    "Method: You map the original input data $x$ into a very high-dimensional feature space $\\phi(x)$ (e.g., using a Gaussian Radial Basis Function kernel). You then perform standard GDA in that high-dimensional space.\\\n",
    "Result: When you project the boundary back to the original 2D space, it appears as a highly complex, non-linear, and irregular contour enclosing the data points.\n",
    "\n",
    "References: \\\n",
    "**Hastie, T., & Tibshirani, R. (1996). Mixture discriminant analysis. Journal of the Royal Statistical Society: Series B (Methodological), 58(1).**\n",
    "\n",
    "**Baudat, G., & Anouar, F. (2000). Generalized discriminant analysis using a kernel approach. Neural Computation, 12(10).**\n",
    "\n",
    "**Friedman, J. H. (1989). Regularized discriminant analysis. Journal of the American Statistical Association, 84(405).**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528daec",
   "metadata": {},
   "source": [
    "### Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e725a",
   "metadata": {},
   "source": [
    "#### Q1.在操作DSM時，我們要如何去選擇noise？能夠讓我們方便計算且結果準確。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53c87d",
   "metadata": {},
   "source": [
    "The standard choice is Gaussian Noise combined with a Geometric Noise Schedule (Annealing).\n",
    "\n",
    "A. Choice of Distribution: Why Gaussian?\\\n",
    "The Result: This allows us to train the neural network simply by trying to predict the noise $z$ (or the clean image $x$) using standard Mean Squared Error (MSE). If you used other distributions (like Cauchy or Uniform), this clean gradient relationship would not exist, making training unstable or computationally intractable.\n",
    "\n",
    "B. Choice of Parameters: Why a Schedule?\\\n",
    "If $\\sigma$ is too small: The model only sees data near the manifold. In the empty space, the estimated gradients are random/inaccurate. When you try to generate data starting from random noise, the model won't know how to \"push\" the point toward the manifold.\\\n",
    "If $\\sigma$ is too large: The noise destroys the data structure. The model learns a blurry \"blob\" rather than detailed features.\n",
    "\n",
    "To maximize both convenience and accuracy, you should set the perturbation kernel as $q_\\sigma(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)$ and train the model conditioning on a variable $\\sigma$ that decreases over time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "References:\\\n",
    "**Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7).**\n",
    "\n",
    "**Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32.**\n",
    "\n",
    "**Hyvärinen, A. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66e9de",
   "metadata": {},
   "source": [
    "#### Q2.我們可以結合DSM與SSM的觀念，將noise與轉換成vector的觀念結合再一起，做出效果可能更好的方法嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef2b27",
   "metadata": {},
   "source": [
    "Based on the evolution of Score-Based Generative Models, the answer is Yes. Combining the concepts of DSM (noise perturbation) and SSM (vector projection/slicing) creates a powerful method, typically known as Sliced Score Matching with Data Perturbation or SSM on Noisy Data.\\\n",
    "\n",
    "The Solution: The Hybrid Approach\n",
    "\n",
    "A. Handling \"Black Box\" or Non-Gaussian Noise\\\n",
    "Result: You can learn to generate data even if the corruption process is complex or unknown (as long as you can sample from it).\n",
    "\n",
    "B. Stabilizing SSM (The Song & Ermon Breakthrough)\n",
    "Problem: Running pure SSM on real images fails because real data sits on a lower-dimensional manifold. The gradients are undefined in the empty space.\\\n",
    "Fix: They \"inflated\" the manifold by adding noise (DSM concept).\n",
    "\n",
    "\n",
    "References:\\\n",
    "**Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32.**\n",
    "\n",
    "**Song, Y., Garg, S., Shi, J., & Ermon, S. (2019). Sliced score matching: A scalable approach to density and score estimation. Proceedings of the 35th Uncertainty in Artificial Intelligence Conference.**\n",
    "\n",
    "**Yoon, S., Lee, G. H., & Hwang, S. J. (2023). Robustifying generation of score-based models via divergence minimization. International Conference on Machine Learning.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcdeaa",
   "metadata": {},
   "source": [
    "### Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f0e8c",
   "metadata": {},
   "source": [
    "#### Q1.大部分SDE長期趨勢會受到drift term影響，那有沒有會受到noise影響的例子?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bc1fc",
   "metadata": {},
   "source": [
    "Yes.\n",
    "\n",
    "This is broadly known as \"Noise-Induced Phenomena.\"\n",
    "\n",
    "A. Geometric Brownian Motion (GBM)\\\n",
    "The Noise Effect:According to Itô's Lemma, the actual long-term trend of the log-price ($\\ln X_t$) is determined by:$$d(\\ln X_t) = (\\mu - \\frac{1}{2}\\sigma^2) dt + \\sigma dW_t$$\\\n",
    "The Result:\n",
    "Even if your drift is positive (e.g., $\\mu = 5\\%$), if the noise is too large (e.g., $\\sigma^2 > 2\\mu$), the effective growth rate $(\\mu - \\frac{1}{2}\\sigma^2)$ becomes negative.\n",
    "Conclusion: High noise causes the system to decay toward 0 in the long run, overpowering the positive drift.\n",
    "\n",
    "B. Noise-Induced Transitions - Physics\\\n",
    "The Result: The long-term behavior (the stationary distribution) is dictated by the noise level. The noise determines whether the system stays in a sub-optimal state or converges to the true global equilibrium (Boltzmann distribution).\n",
    "\n",
    "C. Langevin Dynamics - Generative Models\\\n",
    "The SDE: $dX_t = -\\nabla E(x) dt + \\sqrt{2T} dW_t$\\\n",
    "The Result:\\\n",
    "If Noise = 0: The long-term result is a single point (Optimization).\\\n",
    "If Noise > 0: The long-term result is a Probability Distribution (Sampling).\\\n",
    "Conclusion: Here, the noise changes the fundamental nature of the solution from \"finding a value\" to \"generating a distribution.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "References:\\\n",
    "**Horsthemke, W., & Lefever, R. (1984). Noise-induced transitions: Theory and applications in physics, chemistry, and biology. Springer-Verlag.**\n",
    "\n",
    "**Øksendal, B. (2003). Stochastic differential equations: An introduction with applications (6th ed.). Springer.**\n",
    "\n",
    "**Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd83f96",
   "metadata": {},
   "source": [
    "#### Q2.Brownian motion 對不同SDE系統的長期影響有什麼差異？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e325e",
   "metadata": {},
   "source": [
    "In short, as time $t \\to \\infty$, noise can cause a system to stabilize into a distribution, die out (extinction), or continuously hop between states.\\\n",
    "\n",
    "A. Mean-Reverting Systems (Additive Noise)\\\n",
    "Impact of Brownian Motion: \"Maintenance of Equilibrium\"\\\n",
    "Without Noise: The system would eventually stop exactly at $\\mu$.\\\n",
    "With Noise: The system never stops; it continuously fluctuates around $\\mu$.\\\n",
    "Long-term Result:The system converges to a Gaussian Stationary Distribution.\n",
    "\n",
    "B. Multiplicative Noise Systems\\\n",
    "Long-term Result:\\\n",
    "Low Noise: The system grows exponentially.\\\n",
    "High Noise ($\\sigma^2 > 2\\mu$): The system will almost surely converge to 0 (Extinction), even if the drift $\\mu$ is positive.\\\n",
    "Difference: Here, noise can \"kill\" a system, whereas in the OU process, noise \"sustains\" it.\n",
    "\n",
    "C. Bi-stable / Multi-stable Systems\\\n",
    "Impact of Brownian Motion: \"Ergodicity & Tunneling\"\\\n",
    "Drift: Tries to trap the system in the nearest local minimum (the bottom of one well).\\\n",
    "Noise: Provides the energy to climb over the barrier separating the wells.\\\n",
    "Long-term Result:The system does not settle in a single state. Instead, it converges to a Boltzmann Distribution ($e^{-V(x)/\\sigma^2}$), visiting all stable states over time.\n",
    "\n",
    "\n",
    "References:\\\n",
    "**Khasminskii, R. Z. (2011). Stochastic stability of differential equations (2nd ed.). Springer.**\n",
    "\n",
    "**Evans, L. C. (2013). An introduction to stochastic differential equations. American Mathematical Society.**\n",
    "\n",
    "**Uhlenbeck, G. E., & Ornstein, L. S. (1930). On the theory of the Brownian motion. Physical Review, 36(5), 823–841.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849ee51",
   "metadata": {},
   "source": [
    "### Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fb205",
   "metadata": {},
   "source": [
    "#### Q1. 在 Reverse SDE 的 Euler-Maruyama 求解過程中，$\\Delta t$ 的大小有什麼具體影響？如果 $\\Delta t$ 不夠小，會如何改變最終的數據分佈？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898604f",
   "metadata": {},
   "source": [
    "If $\\Delta t$ is not small enough, it leads to severe Discretization Error, causing the generated data distribution to deviate significantly from the true distribution.\\\n",
    "\n",
    "A. Drift Overshooting and Trajectory Deviation\\\n",
    "Result: The generated images will exhibit geometric distortions, blurred edges, or loss of structural coherence.\n",
    "\n",
    "B. Manifold Deviation\\\n",
    "Result: Once the trajectory enters these regions, the model cannot guide the data back to the correct path, resulting in pure noise or unrecognizable artifacts.\n",
    "\n",
    "C. Noise-Drift Imbalance\\\n",
    "Result: If $\\Delta t$ is chosen poorly, it disrupts the delicate balance between removing noise and injecting noise.\\\n",
    "If $\\Delta t$ is too large, the deterministic drift might overpower the stochastic diffusion (or vice versa), leading to over-smoothed textures or excessive graininess.\\\n",
    "\n",
    "D. Distribution Shift (Statistical Divergence)\\\n",
    "Result:\\\n",
    "Degraded FID Scores: Quantitative metrics for image quality and diversity will worsen significantly.\\\n",
    "Mode Drop: The model may fail to generate diverse samples, capturing only the most common modes of the data while losing rare or detailed examples.\n",
    "\n",
    "\n",
    "References:\\\n",
    "**Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations.**\n",
    "\n",
    "**Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35.**\n",
    "\n",
    "**Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., & Zhu, J. (2022). DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844fdf1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
