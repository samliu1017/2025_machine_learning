{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920949b7",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c05727",
   "metadata": {},
   "source": [
    "#### Q1.Why do we usually begin with a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018194c",
   "metadata": {},
   "source": [
    "##### Answer(By Gemini)\n",
    "\n",
    "\n",
    "Key Advantages of Linear Models\n",
    "\n",
    "Linear models are valuable for several reasons, which make them a popular choice for a wide range of applications:\n",
    "\n",
    "$(1)$ Simplicity and Interpretability\n",
    "\n",
    "$(2)$ Computational Efficiency\n",
    "\n",
    "$(3)$ Well-Understood Statistical Properties\n",
    "\n",
    "$(4)$ Foundation for More Complex Models\n",
    "\n",
    "\n",
    "When to Use a Linear Model\n",
    "\n",
    "A linear model is often the best choice in the following situations:\n",
    "\n",
    "$(1)$ When the relationship is likely linear\n",
    "\n",
    "$(2)$ For baseline performance\n",
    "\n",
    "$(3)$ When interpretability is a priority\n",
    "\n",
    "Limitations of Linear Models\n",
    "\n",
    "Their main limitation is their assumption of a linear relationship between the dependent and independent variables.\n",
    "If the underlying data has a complex, non-linear pattern, a linear model will likely fail to capture it accurately. \n",
    "This can lead to underfitting and poor predictive performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf9420",
   "metadata": {},
   "source": [
    "#### Q2.What is the difference between BGD, SGD, and Mini-Batch Gradient Descent, and what are their advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6afa27",
   "metadata": {},
   "source": [
    "\n",
    "##### Answer 2 (By Chat GPT)\n",
    "$(1)$ Batch Gradient Descent (BGD)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Stable convergence because the gradient is computed using all data.\n",
    "\n",
    "Smooth and predictable updates.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Very slow on large datasets since every step requires going through all data.\n",
    "\n",
    "Not suitable for online or streaming data.\n",
    "\n",
    "Use Case: Small datasets\n",
    "\n",
    "$(2)$ Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Much faster per update because it uses only one example.\n",
    "\n",
    "Can handle very large datasets or streaming data.\n",
    "\n",
    "Can escape local minima more easily due to noisy updates.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Updates are noisy and can fluctuate heavily.\n",
    "\n",
    "Convergence is less stable; may require learning rate decay.\n",
    "\n",
    "Use Case: Very large/streaming data\n",
    "\n",
    "$(3)$ Mini-Batch Gradient Descent\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Faster than BGD because it doesn’t need the full dataset.\n",
    "\n",
    "Less noisy than SGD; more stable convergence.\n",
    "\n",
    "Can exploit optimized matrix operations on hardware (like GPUs).\n",
    "\n",
    "Strikes a balance between speed and accuracy.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Still requires tuning of batch size.\n",
    "\n",
    "Can have some noise in gradient estimate, though usually beneficial.\n",
    "\n",
    "Use Case: Most deep learning tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383078ee",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7529ff",
   "metadata": {},
   "source": [
    "### Q1.Does having more layers or more neurons in a neural network lead to a better result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13798e",
   "metadata": {},
   "source": [
    "Adding more layers (depth) or more neurons per layer (width) both increase the capacity of a neural network, but the effect on performance depends on the task, data, and optimization process.\n",
    "\n",
    "1.More layers (depth)\n",
    "\n",
    "Pros: Capture complex, hierarchical features.\n",
    "\n",
    "Cons: Harder to train, higher compute cost.\n",
    "\n",
    "2.More neurons (width)\n",
    "\n",
    "Pros: Increases expressiveness, easier to train.\n",
    "\n",
    "Cons: More parameters, risk of overfitting.\n",
    "\n",
    "3.Right Model Size\n",
    "\n",
    "Too few parameters → underfitting.\n",
    "\n",
    "Too many parameters → overfitting.\n",
    "\n",
    "It depends on:\n",
    "\n",
    "(1)Dataset size/complexity.\n",
    "\n",
    "(2)Compute budget.\n",
    "\n",
    "(3)Regularization methods .\n",
    "\n",
    "4.Key idea\n",
    "\n",
    "If your model underfits → add more layers/neurons.\n",
    "\n",
    "If your model overfits → reduce size, or add regularization / more data.\n",
    "\n",
    "If training is unstable → depth might be too large without proper techniques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c60f",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82666d52",
   "metadata": {},
   "source": [
    "#### Q1.In class we can approximate every polynoimal by using neural network. Can we use same way to approximate other function e.g. sin(x), cos(x) by using taylor expansion? What may happened, and what is its result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52054f2d",
   "metadata": {},
   "source": [
    "Yes. Since Neural Networks can approximate any polynomial, and Taylor series are polynomials, they can learn the Taylor coefficients.\n",
    "Locality: Taylor expansions are only accurate near the expansion point. As $x$ moves away, the error explodes (polynomials diverge to infinity, while sin/cos are bounded).\n",
    "No Periodicity: Unless a specific periodic activation function (like x + sin(x)^2) is used, a standard NN approximating a polynomial cannot \"learn\" true periodicity, resulting in very poor extrapolation.\n",
    "\n",
    "By **Neural Networks Fail to Learn Periodic Functions and How to Fix It (Ziyin et al., NeurIPS 2020)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553a94f",
   "metadata": {},
   "source": [
    "### Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3263e91",
   "metadata": {},
   "source": [
    "#### Q1.在不同情況下 Generative model 與 discriminative model 表現情況不同，甚麼情況下Generative model 比 discriminative model 好/不好?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca2cd8",
   "metadata": {},
   "source": [
    "Generative Models (e.g., Naive Bayes, GDA): Better for Small Data. They reach their asymptotic error faster (converge with fewer samples).\n",
    "\n",
    "Discriminative Models (e.g., Logistic Regression): Better for Large Data. They have a lower asymptotic error because they do not rely on strict distributional assumptions (like Gaussian), making them more robust to modeling errors.\n",
    "\n",
    "By **On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes (Ng & Jordan, NIPS 2002)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e2145",
   "metadata": {},
   "source": [
    "### Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca3f0c",
   "metadata": {},
   "source": [
    "#### Q1.What may happened when the if the real-world data we used doesn't satisfied normal distribution in GDA model? What is the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad262292",
   "metadata": {},
   "source": [
    "Answer: The model will be Biased.\\\n",
    "While GDA can still estimate parameters (mean, covariance) based on sample moments, the calculated posterior probability $P(y|x)$ will be incorrect.\\\n",
    "This causes the decision boundary to shift, typically resulting in lower accuracy compared to Logistic Regression, which does not assume a specific distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eced063",
   "metadata": {},
   "source": [
    "### Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01592790",
   "metadata": {},
   "source": [
    "#### Q1.數據是高斯分布情況下使用GDA或Logistic regression進行分析，哪個結果會比較準確? 若數據稍微偏離一些結果會是甚麼樣子?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd41047",
   "metadata": {},
   "source": [
    "Strictly Gaussian: GDA is more accurate (it is asymptotically efficient and uses all data information).\n",
    "\n",
    "Slightly Deviated: Logistic Regression is more accurate and Robust (it doesn't assume the data is Gaussian).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54d282",
   "metadata": {},
   "source": [
    "\n",
    "#### Q2.有甚麼方法可以調整GDA的決策邊界，讓其形狀能夠改變成不同樣子？有辦法變成不規則曲線嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b43cb",
   "metadata": {},
   "source": [
    "Standard GDA: Can only produce linear or quadratic boundaries.\n",
    "\n",
    "Methods for Irregular Shapes:\n",
    "\n",
    "Kernel GDA: Use the Kernel Trick to map data to a higher-dimensional space.\n",
    "\n",
    "Gaussian Mixture Models (GMM): Assume each class is composed of multiple Gaussian distributions (rather than just one), allowing the boundary to become complex and irregular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528daec",
   "metadata": {},
   "source": [
    "### Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e725a",
   "metadata": {},
   "source": [
    "#### Q1.在操作DSM時，我們要如何去選擇noise？能夠讓我們方便計算且結果準確。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53c87d",
   "metadata": {},
   "source": [
    "Answer: It is a trade-off.\n",
    "\n",
    "Large Noise: Covers low-density regions and captures global geometric structure, but blurs details.\n",
    "\n",
    "Small Noise: Precise estimation, but unstable in sparse regions.\n",
    "\n",
    "Solution: Modern methods use a Noise Schedule (Annealing), starting with large noise during training and gradually reducing it (e.g., used in NCSN, DDPM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66e9de",
   "metadata": {},
   "source": [
    "#### Q2.我們可以結合DSM與SSM的觀念，將noise與轉換成vector的觀念結合再一起，做出效果可能更好的方法嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef2b27",
   "metadata": {},
   "source": [
    "Yes.\n",
    "\n",
    "Concept: Sliced Score Matching with Denoising. SSM solves the difficulty of computing the Trace in high dimensions (using random projections/vectors), while the \"Denoising\" concept from DSM helps stabilize training and handle noisy data.\n",
    "\n",
    "By **Sliced Score Matching: A Scalable Approach to Density and Score Estimation (Song et al., 2019)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcdeaa",
   "metadata": {},
   "source": [
    "### Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f0e8c",
   "metadata": {},
   "source": [
    "#### Q1.大部分SDE長期趨勢會受到drift term影響，那有沒有會受到noise影響的例子?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bc1fc",
   "metadata": {},
   "source": [
    "Yes.\n",
    "\n",
    "Noise-Induced Transitions (Bistable Systems): Noise can cause a system to jump between two stable states, dominating the long-term state distribution regardless of local drift.\n",
    "\n",
    "Geometric Brownian Motion (High Volatility): If volatility $\\sigma$ is too high ($\\sigma^2/2 > \\mu$), the value will tend toward 0 in the long run, even if the drift $\\mu$ is positive (due to the $-\\sigma^2/2$ term in Ito's Lemma)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd83f96",
   "metadata": {},
   "source": [
    "#### Q2.Brownian motion 對不同SDE系統的長期影響有什麼差異？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e325e",
   "metadata": {},
   "source": [
    "Answer: It primarily affects the Stationary Distribution.\n",
    "\n",
    "Linear Systems (e.g., Ornstein-Uhlenbeck): Noise determines the Variance (width) of the distribution, while drift determines the mean.\n",
    "\n",
    "Non-linear Systems: Noise can alter the system's Topology (e.g., causing Bifurcation) or determine the Residence Time (how long the system stays in a specific state)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
